FROM apache/spark:latest

# Set the PIP_CACHE_DIR to a writable directory
ENV PIP_CACHE_DIR=/tmp/pip-cache

WORKDIR /app/processing
RUN chmod -R 777 /app/processing
RUN chmod -R 777 /app/processing


COPY configs.py utils.py .env /app/processing/
COPY minion/minioClient.py /app/processing/minion/

# Copy the PySpark code and dependencies
COPY processor/dataProcessing.py /app/processing/
COPY processor/requirements.txt /app/processing/

# Install any additional Python packages if needed
USER root
RUN pip3 install -r requirements.txt

# Entry point for running the PySpark job
USER root
ENTRYPOINT ["/opt/spark/bin/spark-submit", "--master", "spark://spark-master:7077", "/app/processing/dataProcessing.py"]
